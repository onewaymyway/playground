/*[IF-FLASH]*/
package tf.train
{

	import tf.AdamOptimizer;

	/**
	 * adam
	 * @param learningRate (number) The learning rate to use for the Adam gradient
              descent algorithm.
	 * @param beta1 (number) The exponential decay rate for the 1st moment estimates.
	 * @param beta2 (number) The exponential decay rate for the 2nd moment estimates.
	 * @param epsilon (number) A small constant for numerical stability.
	 * @return AdamOptimizer
	 */
	public function adam(learningRate:*=null,beta1:*=null,beta2:*=null,epsilon:*=null):AdamOptimizer
	{
		return null;
	}


}
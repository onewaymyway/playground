/*[IF-FLASH]*/
package tf.train
{

	import tf.AdagradOptimizer;

	/**
	 * adagrad
	 * @param learningRate (number) The learning rate to use for the Adagrad gradient
              descent algorithm.
	 * @param initialAccumulatorValue (number) Starting value for the accumulators, must be
              positive.
	 * @return tf.AdagradOptimizer
	 */
	public function adagrad(learningRate:*=null,initialAccumulatorValue:*=null):AdagradOptimizer
	{
		return null;
	}


}